# OpenAI Responses Manifold
**Enables advanced OpenAI features (function calling, tool use, web search, visible reasoning summaries, and more) directly in [Open WebUI](https://github.com/open-webui/open-webui).**

> **Author:** [Justin Kropp](https://github.com/jrkropp)  
> **License:** MIT

âš ï¸ **Version 0.7.0 â€“ Preâ€‘production preview.** The pipe (manifold) is still under early testing and will be fully released as `1.0.0`.

## Installation
1. Copy `openai_responses_manifold.py` to your Open WebUI under **Admin Panel â–¸ Functions**.
2. Enable the pipe and configure the valves for your environment.

## Features

| Feature | Status | Last updated | Notes |
| --- | --- | --- | --- |
| Native function calling | âœ… GA | 2025-06-04 | Automatically enabled for supported models. |
| Visible reasoning summaries | âœ… GA | 2025-06-03 | Available for oâ€‘series models only. |
| Encrypted reasoning tokens | âœ… GA | 2025-06-03 | Persists reasoning context across turns. |
| Optimized token caching | âœ… GA | 2025-06-03 | Save up to ~50â€“75Â % on supported models. |
| Web search tool | âœ… GA | 2025-06-03 | Automatically invoked or toggled manually. |
| Task model support | âœ… GA | 2025-06-06 | Use model as [Open WebUI External Task Model](https://docs.openwebui.com/tutorials/tips/improve-performance-local/) (title generation, tag generation, etc.). |
| Streaming responses (SSE) | âœ… GA | 2025-06-04 | Real-time, partial output streaming for text and tool events. |
| Usage Pass-through | âœ… GA | 2025-06-04 | Tokens and usage aggregated and passed through to Open WebUI GUI. |
| Truncation control | âœ… GA | 2025-06-10 | Valve `TRUNCATION` sets the responses `truncation` parameter (auto or disabled). Works with per-model `max_completion_tokens`. |
| Image input (vision) | ğŸ”„ In-progress | 2025-06-03 | Pending future release. |
| Image generation tool | ğŸ•’ Backlog | 2025-06-03 | Incl. multi-turn image editing (e.g., upload and modify). |
| File upload / file search tool | ğŸ•’ Backlog | 2025-06-03 | Roadmap item. |
| Code interpreter tool | ğŸ•’ Backlog | 2025-06-03 | [OpenAI docs](https://platform.openai.com/docs/guides/tools-code-interpreter) |
| Computer use tool | ğŸ•’ Backlog | 2025-06-03 | [OpenAI docs](https://platform.openai.com/docs/guides/tools-computer-use) |
| Live conversational voice (Talk) | ğŸ•’ Backlog | 2025-06-03 | Requires backend patching; design under consideration. |
| Dynamic chat titles | ğŸ•’ Backlog | 2025-06-03 | For progress/status indication during long tasks. |
| MCP tool support | ğŸ•’ Backlog | 2025-06-09 | Remote MCP servers via Responses API. [More info](https://platform.openai.com/docs/guides/tools-remote-mcp) |


### Other Features
- **Pseudo-models**: `o3-mini-high` / `o4-mini-high` â€“ alias for `o3-mini` / `o4-mini` with high reasoning effort.
- **Debug logging**: Set `LOG_LEVEL` to `debug` for inâ€‘message log details. Can be set globally or per user.
- **Truncation strategy**: Control with the `TRUNCATION` valve. Default `auto` drops middle context when the request exceeds the window; `disabled` fails with a 400 error. Works with each model's `max_completion_tokens` limit.

### Tested models
The manifold should work with any model that supports the responses API. Confirmed with:
| Model ID | Status |
| --- | --- |
| chatgpt-4o-latest | âœ… |
| codex-mini-latest | âœ… |
| gpt-4.1 | âœ… |
| gpt-4o | âœ… |
| o3 | âœ… |
| o3-pro | âœ… |

# How it Works / Design Architecture
## Core concepts
- **Responses API endpoint** â€“ uses the OpenAI Responses API endpoint than completions, enabling features like visible reasoning summaries and built-in tools (web search, etc..).
- **Persistent tool results** â€“ tool outputs are stored alongside messages, making them available on later turns.
- **Encrypted reasoning tokens** â€“ specialized reasoning tokens (`encrypted_content`) are persisted to optimize followâ€‘ups.


Perfect â€” hereâ€™s a polished version of that section with your example embedded and phrasing tightened for clarity and flow:

---

## Persist OpenAI Response Items

OpenAIâ€™s API responses also provides critical non-message items (e.g., reasoning tokens, function calls, and tool outputs). These responses item are provided in an ordered sequence that reflects the model's internal decision-making process.  

**For example:**

```json
[
  {
    "id": "rs_6849f90497fc8192a013fb54f888948c0b902dab32480d90",
    "type": "reasoning",
    "encrypted_content": "[ENCRYPTED_TOKENS_HERE]"
  },
  {
    "type": "function_call",
    "function_call": {
      "name": "get_weather",
      "arguments": {
        "location": "New York"
      }
    }
  },
  {
    "type": "function_call_result",
    "function_result": {
      "location": "New York",
      "temperature": "72Â°F",
      "condition": "Sunny"
    }
  },
  {
    "type": "message",
    "role": "assistant",
    "content": "Itâ€™s currently 72Â°F and sunny in New York."
  }
]
```

Storing only the final assistant message discards the context that produced it. By contrast, appending all response items (in the order they were produced) ensures:

* **Precise context reconstruction**
* **Reduced latency** (reasoning doesnâ€™t have to be re-generated)
* **Improved cache utilization** (up to 75% cost savings)

**Thus, we face a challenge:**
While direct persistence in Open WebUI (e.g., via `Chats.update_chat_by_id()`) can store metadata, this approach bypasses Open WebUIâ€™s extensible filter pipeline. Any filters that modify `body["messages"]` before your pipe runs wonâ€™t be reflected if you regenerate context directly from the database.

Ideally, context should be reconstructed from the exact `body["messages"]` structure passed into your pipeâ€”after filters have had a chance to manipulate.

```json
body = {
  "messages": [
    { "role": "system", "content": "System prompt text..." },
    { "role": "user", "content": "User question..." }
  ]
}
```

These messages contain only `role` and `content`.  To bridge this gap, our solution invisibly encodes metadata references (short IDs) directly into the `content`, using zero-width characters and stores the full unmodified OpenAI response JSON using `Chats.update_chat_by_id()`.  On subsequent API calls, the pipeline decodes the hidden zero-width characters within the messages, retrieves the corresponding metadata from the database, and reconstructs the original conversational history accurately and in the precise order it occurred.

**Why not encode the entire metadata directly?**
Encoding full OpenAI response items directly into zero-width characters significantly increases storage consumption. Instead, encoding only a short, unique identifier greatly optimizes storage while enabling full metadata retrieval.

This method combines seamless compatibility with Open WebUI's filter pipeline, preserves conversation fidelity, and optimizes storage usage effectively.


**Pro Tip**
You can inspect the DB chat item directly in your browser by opening **Developer Tools** and examining the POST request for a chat in the **Network** tab.

Full chat JSON structure example:

```json
{
  "id": "<chat_id>",
  "user_id": "<user_id>",
  "title": "<chat_title>",
  "chat": {
    "id": "<chat_internal_id>",
    "title": "<chat_title>",
    "models": ["<model_id>"],
    "params": {},
    "history": {
      "messages": {
        "<message_id>": {
          "id": "<message_id>",
          "parentId": "<parent_message_id_or_null>",
          "childrenIds": ["<child_message_id>", "..."],
          "role": "user|assistant|function",
          "content": "<message_text_or_null>",
          "model": "<model_id>",
          "modelName": "<model_display_name>",
          "modelIdx": <index>,
          "timestamp": <unix_ms>,
          "usage": {},
          "done": true
        }
      },
      "currentId": "<current_message_id>"
    },
    "messages": [
      {
        // Flattened version of messages
      }
    ],
    "tags": ["<optional_tag>", "..."],
    "timestamp": <unix_ms>,
    "files": [
      // Any attached files
    ],

    // â€”â€” Custom Extension: Added by openai_responses_pipe â€”â€”
    "openai_responses_pipe": {
      "__v": 2,
      "messages": {
        "<message_id>": {
          "model": "<model_that_generated_nonmessage_items>",
          "created_at": <unix_timestamp>,
          "items": [
            {
              "type": "function_call|function_call_result|reasoning|...",
              "...": "..."
            }
          ]
        }
      }
    }
    // â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”
  },
  "updated_at": <unix_timestamp>,
  "created_at": <unix_timestamp>,
  "share_id": null,
  "archived": false,
  "pinned": false,
  "meta": {},
  "folder_id": null
}
```
